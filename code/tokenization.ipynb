{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 경고 출력 방지\n",
    "# import warnings \n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93516ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 패키지 로드\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b162178",
   "metadata": {},
   "source": [
    "## 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c890948",
   "metadata": {},
   "source": [
    "### NLTK natural language toolkit 은 교육용으로  개발된 자연어 처리 및 문서 분석용 파이썬 라이브러리\n",
    "- 주요 기능\n",
    "    - 말뭉치\n",
    "    - 토큰생성\n",
    "    - 형태소 분석\n",
    "    - 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b110204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d186ce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() #전부 다운받으면 속편하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c7e8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is', 'it', 'possible', 'distinguishing', 'cats', 'and', 'dogs']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"Is it possible distinguishing cats and dogs\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34a82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('possible', 'JJ'),\n",
       " ('distinguishing', 'VBG'),\n",
       " ('cats', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('dogs', 'NNS')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VBZ 동사, 동명사 또는 현재 분사, PRP 인칭 대명사, JJ 형용사,VBG 동사, 동명사 또는 현재 분사, NNS 명사 복수형, CC 등위 접속사\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288d3a9",
   "metadata": {},
   "source": [
    "### KoNLPy 한국어 처리를 위한 파이썬 라이브러리 (JDK가 요구됨)\n",
    "- 주요 기능\n",
    "    - 형태소 분석\n",
    "    - 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c54370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥러닝이', '뭐', 'ㄴ가요', '?', '어렵', '나', '?']\n"
     ]
    }
   ],
   "source": [
    "# 형태소로 변환\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "print(komoran.morphs('딥러닝이 뭔가요? 어렵나?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19312f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('소파', 'NNP'), ('위', 'NNG'), ('에', 'JKB'), ('있', 'VV'), ('는', 'ETM'), ('게', 'EC'), ('개', 'VV'), ('냐', 'EC'), ('냥', 'NNB'), ('이', 'VCP'), ('냐', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "#품사 태깅\n",
    "print(komoran.pos('소파 위에 있는게 개냐 냥이냐'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f919a",
   "metadata": {},
   "source": [
    "### Gensim Word2Vec 라이브러리\n",
    "- 주요 기능\n",
    "    - 임베딩: 워드 투벡터\n",
    "    - 토픽 모델링: 문서집합에서 추상적인 주제를 발견하기 위한 통계적 모델 중 하나, 텍스트 본문의 숨겨진 의미 구조를 발견하는데 사용 되는 텍스트 마이닝 기법  ==> 각 주제별로 단어 표현을 묶어 주는것\n",
    "    - LDA(lATENT dIRICHLET Allocation) 주어진 문서에 대해 각문서에 어떤주제들이 존대하는지 서술하는 확률적 토픽모델 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e472b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c46e2b37",
   "metadata": {},
   "source": [
    "### sklearn 자연어에서 측성 추출용\n",
    "- 주요 기능\n",
    "    - ConuntVectorizer : 텍스트에서 단어의 등장 횟루를 기준으로 특성을 추출\n",
    "    - Tfidfvectorizer : TF-IDF 값을 이용해 텍스트 특징 추출\n",
    "    - HashingVectorizer 카운트벡터와 같지만 해싱함수로 처리시간 줄어듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d4483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a16a3b6d",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "토큰화 에서 고려해야할 사항\n",
    "1) 구두점이나 특수 문자를 단순 제외해서는 안 된다. ->  예시) Ph.D나 AT&T  123,456,789 $45.55\n",
    "2) 줄임말과 단어 내에 띄어쓰기가 있는 경우."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8907a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize,WordPunctTokenizer,TreebankWordTokenizer\n",
    "text_sample1 = 'The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.'\n",
    "text_sample2 = '안녕하세요. 저는 홍길동 입니다. 팀에서 도적을 담당하고 있습니다.'\n",
    "text_sample3 = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127a2cae",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c42324a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'OpenAI', 'Python', 'library', 'provides', 'convenient', 'access', 'to', 'the', 'OpenAI', 'REST', 'API', 'from', 'any', 'Python', '3.8+', 'application', '.', 'The', 'library', 'includes', 'type', 'definitions', 'for', 'all', 'request', 'params', 'and', 'response', 'fields', ',', 'and', 'offers', 'both', 'synchronous', 'and', 'asynchronous', 'clients', 'powered', 'by', 'httpx', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text_sample1)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "173998bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treebankTokenizer ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "#Penn Treebank Tokenization의 규칙\n",
    "#규칙 1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "#규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다.\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print('treebankTokenizer',tokenizer.tokenize(text_sample1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5de07f",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "보통 갖고있는 코퍼스가 정제되지 않은 상태라면, 코퍼스는 문장 단위로 구분되어 있지 않아서 이를 사용하고자 하는 용도에 맞게 문장 토큰화가 필요할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253e9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+ application.',\n",
       " 'The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences  = sent_tokenize(text_sample1)\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d15ae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요.', '저는 홍길동 입니다.', '팀에서 도적을 담당하고 있습니다.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences  = sent_tokenize(text_sample2)\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2931fb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Starting a home-based restaurant may be an ideal.',\n",
       " \"it doesn't have a food chain or restaurant of their own.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences  = sent_tokenize(text_sample3)\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fdeb52",
   "metadata": {},
   "source": [
    "### 한글어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dabf9d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', '.', '저', '는', '홍길동', '입니다', '.', '팀', '에서', '도적', '을', '담당', '하고', '있습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 한글 토큰화\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(okt.morphs(text_sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbd90ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('안녕하세요', 'Adjective'), ('.', 'Punctuation'), ('저', 'Noun'), ('는', 'Josa'), ('홍길동', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation'), ('팀', 'Noun'), ('에서', 'Josa'), ('도적', 'Noun'), ('을', 'Josa'), ('담당', 'Noun'), ('하고', 'Josa'), ('있습니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(text_sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc7fa5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['저', '홍길동', '팀', '도적', '담당']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(text_sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45738a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['홍길동', '도적', '담당']\n"
     ]
    }
   ],
   "source": [
    "print(okt.phrases(text_sample2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
